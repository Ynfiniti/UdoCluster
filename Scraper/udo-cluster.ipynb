{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "apiKey = \"hTLGygty2Sj5IB368rcArA63Xu29hW2r\"\n",
    "archiveUrl = f'https://api.nytimes.com/svc/archive/v1/#1/#2.json?api-key={apiKey}'\n",
    "\n",
    "def parseDoc(doc) -> list[str]:\n",
    "  headline = doc[\"headline\"][\"print_headline\"] or doc[\"headline\"][\"main\"]\n",
    "  leadPara = doc[\"lead_paragraph\"]\n",
    "  return str(headline + (\" \" if leadPara else \"\") + leadPara)\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    # Define a function to split tokens based on custom rules\n",
    "    def split_tokens(doc):\n",
    "        new_tokens = []\n",
    "        for token in doc:\n",
    "            if token.pos == ADJ or token.pos == NOUN:\n",
    "                new_tokens.extend([t.text for t in nlp(token.text)])\n",
    "            else:\n",
    "                new_tokens.append(token.text)\n",
    "        return Doc(nlp.vocab, words=new_tokens)\n",
    "    \n",
    "    return split_tokens\n",
    "\n",
    "# Disabling components not needed (optional, but useful if run on a large dataset)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "#nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "nlp.add_pipe(\"merge_noun_chunks\")\n",
    "nlp.add_pipe(\"merge_entities\")\n",
    "#nlp.add_pipe(\"lemmatizer\", config= {\"mode\": \"lookup\"})\n",
    "\n",
    "# Load the larger model for similarity calculation\n",
    "nlp_lg = nlp\n",
    "\n",
    "# Example text\n",
    "text = \"German Chancellor Angela Merkel died in 1936 in New York. She got shot by a mysterious terrorist. terror is bad. terrorism is really not cool.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.nytimes.com/svc/archive/v1/2010/9.json?api-key=hTLGygty2Sj5IB368rcArA63Xu29hW2r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hide killed pakistan statement malaysia congress plot alabama suits statement malaysia congress killed'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fetch\n",
    "\n",
    "print(archiveUrl.replace(\"#1\", \"2010\").replace(\"#2\", \"9\"))\n",
    "res = requests.get(archiveUrl.replace(\"#1\", \"2010\").replace(\"#2\", \"9\"))\n",
    "obj = json.loads(res.text)\n",
    "with open(\"september.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json_file.write(res.text)\n",
    "#words = \" \".join(map(lambda doc: parseDoc(doc), list(obj[\"response\"][\"docs\"])))\n",
    "# Test for clustering words in Caps\n",
    "words = \"HIDE KILLED PAKISTAN STATEMENT MALAYSIA CONGRESS PLOT ALABAMA SUITS STATEMENT MALAYSIA CONGRESS Killed\".lower()\n",
    "\n",
    "words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\loren\\OneDrive\\Desktop\\Big Data Analytics\\UdoCluster\\Scraper\\udo-cluster.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/loren/OneDrive/Desktop/Big%20Data%20Analytics/UdoCluster/Scraper/udo-cluster.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m wordCount \u001b[39m=\u001b[39m {}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/loren/OneDrive/Desktop/Big%20Data%20Analytics/UdoCluster/Scraper/udo-cluster.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m matches: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mdict\u001b[39m] \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/loren/OneDrive/Desktop/Big%20Data%20Analytics/UdoCluster/Scraper/udo-cluster.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m doc_trf \u001b[39m=\u001b[39m nlp(text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/loren/OneDrive/Desktop/Big%20Data%20Analytics/UdoCluster/Scraper/udo-cluster.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc_trf:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/loren/OneDrive/Desktop/Big%20Data%20Analytics/UdoCluster/Scraper/udo-cluster.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mtext \u001b[39min\u001b[39;00m matches\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\language.py:1037\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[0;32m   1017\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1018\u001b[0m     text: Union[\u001b[39mstr\u001b[39m, Doc],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     component_cfg: Optional[Dict[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Any]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1022\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Doc:\n\u001b[0;32m   1023\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \u001b[39m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[39m    is preserved.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[39m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1037\u001b[0m     doc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_doc(text)\n\u001b[0;32m   1038\u001b[0m     \u001b[39mif\u001b[39;00m component_cfg \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1039\u001b[0m         component_cfg \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\language.py:1128\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[39mreturn\u001b[39;00m doc_like\n\u001b[0;32m   1127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(doc_like, \u001b[39mstr\u001b[39m):\n\u001b[1;32m-> 1128\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_doc(doc_like)\n\u001b[0;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(doc_like, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1130\u001b[0m     \u001b[39mreturn\u001b[39;00m Doc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\u001b[39m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\language.py:1120\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length:\n\u001b[0;32m   1117\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1118\u001b[0m         Errors\u001b[39m.\u001b[39mE088\u001b[39m.\u001b[39mformat(length\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(text), max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length)\n\u001b[0;32m   1119\u001b[0m     )\n\u001b[1;32m-> 1120\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(text)\n",
      "\u001b[1;32mc:\\Users\\loren\\OneDrive\\Desktop\\Big Data Analytics\\UdoCluster\\Scraper\\udo-cluster.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/loren/OneDrive/Desktop/Big%20Data%20Analytics/UdoCluster/Scraper/udo-cluster.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m new_tokens \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/loren/OneDrive/Desktop/Big%20Data%20Analytics/UdoCluster/Scraper/udo-cluster.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/loren/OneDrive/Desktop/Big%20Data%20Analytics/UdoCluster/Scraper/udo-cluster.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39;49mpos \u001b[39m==\u001b[39m ADJ \u001b[39mor\u001b[39;00m token\u001b[39m.\u001b[39mpos \u001b[39m==\u001b[39m NOUN:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/loren/OneDrive/Desktop/Big%20Data%20Analytics/UdoCluster/Scraper/udo-cluster.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         new_tokens\u001b[39m.\u001b[39mextend([t\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m nlp(token\u001b[39m.\u001b[39mtext)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/loren/OneDrive/Desktop/Big%20Data%20Analytics/UdoCluster/Scraper/udo-cluster.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'pos'"
     ]
    }
   ],
   "source": [
    "## Search\n",
    "\n",
    "def checkMatches(token, relevant_words):\n",
    "    token_lg = nlp_lg(token.text)\n",
    "    for comp in relevant_words.keys():\n",
    "        comp_lg = nlp_lg(comp)\n",
    "        similarity = token_lg.similarity(comp_lg)\n",
    "        if(similarity >= 0.9):\n",
    "            return (comp, similarity)\n",
    "    return False\n",
    "\n",
    "# Task 1: Extracting relevant words using the transformer-based model\n",
    "wordCount = {}\n",
    "matches: dict[str, dict] = {}\n",
    "doc_trf = nlp(text)\n",
    "for token in doc_trf:\n",
    "    if token.text in matches.keys():\n",
    "        matches[token.text][\"amount\"] += 1\n",
    "    elif not token.is_stop and not token.is_punct:\n",
    "        matched = checkMatches(token, wordCount)\n",
    "        if(not matched):\n",
    "            wordCount[token.text] = {}\n",
    "            wordCount[token.text][\"amount\"] = 1\n",
    "        else:\n",
    "            if token.text in matches.keys():\n",
    "                matches[token.text][\"amount\"] = matches[token.text][\"amount\"] + 1\n",
    "            else:\n",
    "                matches[token.text] = {\n",
    "                    \"match\": matched[0],\n",
    "                    # This might be removed later\n",
    "                    \"similarity\": matched[1],\n",
    "                    \"amount\": 1\n",
    "                }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'German Chancellor Angela Merkel': {'amount': 1},\n",
       " 'died': {'amount': 1},\n",
       " '1936': {'amount': 1},\n",
       " 'New York': {'amount': 1},\n",
       " 'got': {'amount': 1},\n",
       " 'shot': {'amount': 1},\n",
       " 'a mysterious terrorist': {'amount': 1},\n",
       " 'terror': {'amount': 1},\n",
       " 'bad': {'amount': 1},\n",
       " 'terrorism': {'amount': 1},\n",
       " 'cool': {'amount': 1}}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for m in matches.items():\n",
    "    if \"duplicates\" not in wordCount[m[1][\"match\"]]:\n",
    "        wordCount[m[1][\"match\"]][\"duplicates\"] = {}\n",
    "    wordCount[m[1][\"match\"]][\"duplicates\"][m[0]] = {\n",
    "        \"amount\": m[1][\"amount\"],\n",
    "        \"similarity\": m[1][\"similarity\"]\n",
    "    }\n",
    "    wordCount[m[1][\"match\"]][\"amount\"] = wordCount[m[1][\"match\"]][\"amount\"] + m[1][\"amount\"]\n",
    "\n",
    "wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'German Chancellor Angela Merkel': {'amount': 1}, 'died': {'amount': 1}, '1936': {'amount': 1}, 'New York': {'amount': 1}, 'got': {'amount': 1}, 'shot': {'amount': 1}, 'a mysterious terrorist': {'amount': 1}, 'terror': {'amount': 1}, 'bad': {'amount': 1}, 'terrorism': {'amount': 1}, 'cool': {'amount': 1}}\n",
      "\n",
      "--------------------\n",
      "\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(wordCount)\n",
    "print(\"\\n--------------------\\n\")\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taggers\n",
      "['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\loren\\OneDrive\\Desktop\\Big Data Analytics\\UdoCluster\\Scraper\\udo-cluster.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/loren/OneDrive/Desktop/Big%20Data%20Analytics/UdoCluster/Scraper/udo-cluster.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m tagger \u001b[39min\u001b[39;00m taggers:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/loren/OneDrive/Desktop/Big%20Data%20Analytics/UdoCluster/Scraper/udo-cluster.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tagger[\u001b[39m\"\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/loren/OneDrive/Desktop/Big%20Data%20Analytics/UdoCluster/Scraper/udo-cluster.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtag[\u001b[39m\"\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mspacy\u001b[39m.\u001b[39mexplain(tag)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "taggers = [{\n",
    "    \"name\": \"Taggers\",\n",
    "    \"values\": [\n",
    "        \"$\", \"''\", \",\", \"-LRB-\", \"-RRB-\", \".\", \":\", \"ADD\", \"AFX\", \"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"HYPH\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NFP\", \"NN\", \"NNP\", \"NNPS\", \"NNS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"SYM\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\", \"XX\", \"_SP\", \"``\"\n",
    "    ]\n",
    "}]\n",
    "\n",
    "for tagger in taggers:\n",
    "    print(tagger[\"name\"])\n",
    "    print(tagger[\"values\"])\n",
    "\n",
    "for tagger in taggers:\n",
    "\n",
    "    for tag in tagger[\"values\"]:\n",
    "        print(f\"{tag[\"name\"]}: {spacy.explain(tag)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a mysterious terrorist terror 0.4993249475955963\n",
      "a mysterious terrorist terrorism 0.5030153393745422\n",
      "a mysterious terrorist really 0.4049932062625885\n",
      "terror a mysterious terrorist 0.4993249475955963\n",
      "terror terrorism 0.8408539891242981\n",
      "terrorism a mysterious terrorist 0.5030153393745422\n",
      "terrorism terror 0.8408539891242981\n",
      "really a mysterious terrorist 0.4049932062625885\n",
      "really not 0.5845474600791931\n",
      "really cool 0.4084397852420807\n",
      "not really 0.5845474600791931\n",
      "cool really 0.4084397852420807\n",
      "German Chancellor Angela Merkel\n",
      "die\n",
      "in\n",
      "1936\n",
      "in\n",
      "New York\n",
      ".\n",
      "she\n",
      "get\n",
      "shoot\n",
      "by\n",
      "a mysterious terrorist\n",
      ".\n",
      "terror\n",
      "be\n",
      "bad\n",
      ".\n",
      "terrorism\n",
      "be\n",
      "really\n",
      "not\n",
      "cool\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "x = nlp_lg(text)\n",
    "for i in x:\n",
    "    for j in x:\n",
    "        if i.similarity(j) >0.4 and i.similarity(j)<1:\n",
    "            print(i,j, i.similarity(j))\n",
    "\n",
    "for i in x:\n",
    "    print(i.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there PRON\n",
      "be VERB\n",
      "terror NOUN\n",
      ". PUNCT\n",
      "terrorism NOUN\n",
      "be AUX\n",
      "bad ADJ\n",
      ". PUNCT\n",
      "there PRON\n",
      "be VERB\n",
      "terrorist NOUN\n"
     ]
    }
   ],
   "source": [
    "my_text = \"i ate food graciously. he has eaten. she eats. they eat\"\n",
    "my_text= \"there was terror. terrorism is bad. there were terrorists\"\n",
    "x = nlp(my_text)\n",
    "for i in x:\n",
    "    print(i.lemma_, i.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German PROPN\n",
      "Chancellor PROPN\n",
      "Angela PROPN\n",
      "Merkel PROPN\n",
      "died VERB\n",
      "in ADP\n",
      "1936 NUM\n",
      "in ADP\n",
      "New PROPN\n",
      "York PROPN\n",
      ". PUNCT\n",
      "She PRON\n",
      "got AUX\n",
      "shot VERB\n",
      "by ADP\n",
      "a DET\n",
      "mysterious ADJ\n",
      "terrorist NOUN\n",
      ". PUNCT\n",
      "terror NOUN\n",
      "is AUX\n",
      "bad ADJ\n",
      ". PUNCT\n",
      "terrorism NOUN\n",
      "is AUX\n",
      "really ADV\n",
      "not PART\n",
      "cool ADJ\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "my_text = \"German Chancellor Angela Merkel died in 1936 in New York. She got shot by a mysterious terrorist. terror is bad. terrorism is really not cool.\"\n",
    "doc = nlp(my_text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
